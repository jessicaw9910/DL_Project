{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78cd8728",
   "metadata": {},
   "source": [
    "# ChemVAE Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d3bbdfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1a4f2d",
   "metadata": {},
   "source": [
    "## Import and Pre-Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "773495e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "args_input = '../data/zinc.csv'\n",
    "args_train = 100000\n",
    "args_val = 1000\n",
    "args_output = True\n",
    "args_colc = 'SMILES'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "779fc2f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 0 training index conversion errors\n",
      "There are 0 testing index conversion errors\n",
      "\n",
      "There are 0 training one-hot conversion errors\n",
      "There are 0 testing one-hot conversion errors\n"
     ]
    }
   ],
   "source": [
    "# GitHub data directory contains only tar.gz version\n",
    "# import pandas as pd\n",
    "# df = pd.read_csv('../data/zinc.tar.gz', compression='gzip', header=0, sep=',', error_bad_lines=False)\n",
    "# df.columns[0] = 'SMILES'\n",
    "\n",
    "df = import_data(args_input)\n",
    "X_train, X_test = return_splits(df, n_train=args_train, n_test=args_val, col_chem=args_colc)\n",
    "char2idx, idx2char, train_idx, test_idx = create_data(X_train, X_test, colname=args_colc)   \n",
    "train_oh, test_oh = check_conversions(idx2char, train_idx, X_train, test_idx, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a77e6464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "print(max([len(i) for i in X_train]))\n",
    "print(max([len(i) for i in X_test]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5cb5d87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7d43ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7e4b6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6acba0d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7e48fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c9aafca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_oh.shape[1] ## d_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7879fd1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_oh.shape[2] ## d_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "6b6908e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChemVAE(nn.Module):\n",
    "    def __init__(self, d_input, d_output):\n",
    "        super(ChemVAE, self).__init__()\n",
    "\n",
    "        self.conv_1 = nn.Conv1d(d_input, 9, kernel_size=9)\n",
    "        self.conv_2 = nn.Conv1d(9, 9, kernel_size=9)\n",
    "        self.conv_3 = nn.Conv1d(9, 10, kernel_size=11)\n",
    "        self.linear_0 = nn.Linear(30, 435)\n",
    "        self.linear_1 = nn.Linear(435, 292)\n",
    "        self.linear_2 = nn.Linear(435, 292)\n",
    "\n",
    "        self.linear_3 = nn.Linear(292, 292)\n",
    "        self.gru = nn.GRU(292, 501, 3, batch_first=True)\n",
    "        self.linear_4 = nn.Linear(501, d_output)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax()\n",
    "\n",
    "    def encode(self, x):\n",
    "        x = self.relu(self.conv_1(x))\n",
    "        x = self.relu(self.conv_2(x))\n",
    "        x = self.relu(self.conv_3(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.selu(self.linear_0(x))\n",
    "        z_mu = self.linear_1(x)\n",
    "        z_logvar = self.linear_2(x)\n",
    "        return z_mu, z_logvar\n",
    "\n",
    "    def sampling(self, mu, log_var):\n",
    "        '''\n",
    "        Sample from latent space, z ~ N(μ, σ**2)\n",
    "        '''\n",
    "        sigma = torch.exp(log_var / 2)\n",
    "\n",
    "        #epsilon = torch.randn(sigma.size()).float()\n",
    "        epsilon = torch.randn_like(sigma).float()\n",
    "\n",
    "        self.z_mean = mu\n",
    "        self.z_logvar = log_var\n",
    "\n",
    "        # use the reparameterization trick\n",
    "        return mu + sigma * epsilon\n",
    "\n",
    "    def decode(self, z):\n",
    "        z = F.selu(self.linear_3(z))\n",
    "        z = z.view(z.size(0), 1, z.size(-1)).repeat(1, 60, 1)\n",
    "        output, hn = self.gru(z)\n",
    "        out_reshape = output.contiguous().view(-1, output.size(-1))\n",
    "        y0 = F.softmax(self.linear_4(out_reshape), dim=1)\n",
    "        y = y0.contiguous().view(output.size(0), -1, y0.size(-1))\n",
    "        return y\n",
    "\n",
    "    def forward(self, x):\n",
    "        z_mean, z_logvar = self.encode(x)\n",
    "        z = self.sampling(z_mean, z_logvar)\n",
    "        output = self.decode(z)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "b128af21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_kld(z_mean, z_logvar):\n",
    "    '''\n",
    "    Calculate KL divergence\n",
    "    '''\n",
    "    return -0.5 * torch.mean(1 + z_logvar - z_mean**2 - z_logvar.exp())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c76aa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 / 1\t16609.7051\n",
      "train tensor(0.1661, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "KLD: 3.7999\n",
      "20 / 1\t12087.0645\n",
      "train tensor(0.1209, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "KLD: 4.0856\n",
      "30 / 1\t9504.0215\n",
      "train tensor(0.0950, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "KLD: 3.9255\n",
      "40 / 1\t8807.6455\n",
      "train tensor(0.0881, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "KLD: 3.8718\n",
      "50 / 1\t12133.7646\n",
      "train tensor(0.1213, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "KLD: 4.1607\n",
      "60 / 1\t7215.5122\n",
      "train tensor(0.0722, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "KLD: 3.9230\n"
     ]
    }
   ],
   "source": [
    "args_lr = 0.001\n",
    "args_dynlr = True\n",
    "args_batch_size = 200\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = \"cpu\"\n",
    "\n",
    "if args_dynlr:\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', \n",
    "                                                     factor = 0.8, \n",
    "                                                     patience = 3,\n",
    "                                                     min_lr = 0.0001)\n",
    "\n",
    "X_train = torch.from_numpy(train_oh.astype(np.float32))\n",
    "X_test = torch.from_numpy(test_oh.astype(np.float32))\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# train_loader = torch.utils.data.DataLoader(X_train, batch_size=200, shuffle=True)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(X_train, \n",
    "                                           batch_size=args_batch_size,\n",
    "                                           shuffle=True, \n",
    "                                           num_workers=6,\n",
    "                                           drop_last = True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(X_test, \n",
    "                                          batch_size=args_batch_size,\n",
    "                                          shuffle=True, \n",
    "                                          num_workers=6,\n",
    "                                          drop_last = True)\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "model = ChemVAE(train_oh.shape[1], train_oh.shape[2]).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr = args_lr)\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, data in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        kld = calc_kld(model.z_mean, model.z_logvar)\n",
    "        loss = F.binary_cross_entropy(output, data, size_average=False) + kld\n",
    "        loss.backward()\n",
    "        train_loss += loss\n",
    "        optimizer.step()\n",
    "        if epoch % 10 == 0 and batch_idx == 0:\n",
    "            print(f'{epoch} / {batch_idx + 1}\\t{loss:.4f}')\n",
    "            print('train', train_loss / len(train_loader.dataset))\n",
    "            print(f'KLD: {kld:.4f}')\n",
    "    return train_loss / len(train_loader.dataset)\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train_loss = train(epoch)\n",
    "\n",
    "#     if batch_idx==0:\n",
    "#           inp = data.cpu().numpy()\n",
    "#           outp = output.cpu().detach().numpy()\n",
    "#           lab = data.cpu().numpy()\n",
    "#           print(\"Input:\")\n",
    "#           print(decode_smiles_from_indexes(map(from_one_hot_array, inp[0]), charset))\n",
    "#           print(\"Label:\")\n",
    "#           print(decode_smiles_from_indexes(map(from_one_hot_array, lab[0]), charset))\n",
    "#           sampled = outp[0].reshape(1, 120, len(charset)).argmax(axis=2)[0]\n",
    "#           print(\"Output:\")\n",
    "#           print(decode_smiles_from_indexes(sampled, charset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "4393f31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "80dc7fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_idx = list(output.argmax(axis=2).cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "95d8e1b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([25, 11, 11, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19,\n",
       "       19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19,\n",
       "        4, 26, 26, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_idx[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "0611c56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_char = [convert_num2str(i, idx2char) for i in output_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "3c7fb331",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CCccccccccccccccccccccccccccccccc1',\n",
       " 'CCcccccccccccccccccccccccccccccccccc1',\n",
       " 'CCcccccccccccccccccccccccccccccccc',\n",
       " 'CCcccccccccccccccccccccccc1',\n",
       " 'CCcccccccccccccccccccccccccccccccc',\n",
       " 'CCccccccccccccccccccccccccc1',\n",
       " 'CCcccccccccccccccccccccc11',\n",
       " 'CCccccccccccccccccccccccc1',\n",
       " 'CCccccccccccccccccccccccccccccc',\n",
       " 'CCccccccccccccccccccccccccc1',\n",
       " 'CCcccccccccccccccccccccccccccccc1',\n",
       " 'CCccccccccccccccccccccccccccc1',\n",
       " 'CCccccccccccccccccccccccccccc1',\n",
       " 'CCccccccccccccccccccccccccccccccccccc1',\n",
       " 'CCcccccccccccccccccccccccccccccccc',\n",
       " 'CCcccccccccccccccccccccccccc1',\n",
       " 'CCccccccccccccccccccccccc1',\n",
       " 'CCcccccccccccccccccccccccccccccc',\n",
       " 'CCccccccccccccccccccccccccccccccccc1',\n",
       " 'CCccccccccccccccccccccccccccc1',\n",
       " 'CCcccccccccccccccccccccccccccc1',\n",
       " 'CCccccccccccccccccccccccccccc1',\n",
       " 'CCccccccccccccccccccccccccccc1',\n",
       " 'CCcccccccccccccccccccccccccc1',\n",
       " 'CCccccccccccccccccccccccccccccc',\n",
       " 'CCcccccccccccccccccccccccccc1',\n",
       " 'CCccccccccccccccccccccccccccc1',\n",
       " 'CCcccccccccccccccccccccccccccccccccc1',\n",
       " 'CCccccccccccccccccccccccccccccc',\n",
       " 'CCccccccccccccccccccccccccccc1',\n",
       " 'CCccccccccccccccccccccccccccccccccc1',\n",
       " 'CCccccccccccccccccccccccccccccccccccc',\n",
       " 'CCccccccccccccccccccccccccccccccccc1',\n",
       " 'CCccccccccccccccccccccccccc1',\n",
       " 'CCcccccccccccccccccccccccccccccc',\n",
       " 'CCcccccccccccccccccccccccccccc',\n",
       " 'CCccccccccccccccccccccccccccc1',\n",
       " 'CCcccccccccccccccccccccccccccccc',\n",
       " 'CCccccccccccccccccccccccccccccc',\n",
       " 'CCcccccccccccccccccccccccccc1',\n",
       " 'CCccccccccccccccccccccccc1',\n",
       " 'CCcccccccccccccccccccccccc1',\n",
       " 'CCcccccccccccccccccccccccccccc1',\n",
       " 'CCcccccccccccccccccccccccccccccc1',\n",
       " 'CCccccccccccccccccccccccccccc1',\n",
       " 'CCcccccccccccccccccccccccccccccc1',\n",
       " 'CCccccccccccccccccccccccccccccccc1',\n",
       " 'CCccccccccccccccccccccccc1',\n",
       " 'CCccccccccccccccccccccccccccccccc1',\n",
       " 'CCccccccccccccccccccccccccccccccccc',\n",
       " 'CCcccccccccccccccccccccccccccccccc1',\n",
       " 'CCcccccccccccccccccccccc1',\n",
       " 'CCcccccccccccccccccccccccc1',\n",
       " 'CCcccccccccccccccccccccccccccc1',\n",
       " 'CCcccccccccccccccccccccccccccc1',\n",
       " 'CCcccccccccccccccccccccccccccc1',\n",
       " 'CCccccccccccccccccccccccccc1',\n",
       " 'CCccccccccccccccccccccccccccc1',\n",
       " 'CCcccccccccccccccccccccccccccccc',\n",
       " 'CCccccccccccccccccccccccccccc1',\n",
       " 'CCcccccccccccccccccccccccccccccc',\n",
       " 'CCcccccccccccccccccccccccccc1',\n",
       " 'CCcccccccccccccccccccccccccccc1',\n",
       " 'CCccccccccccccccccccccccccccccccccc1',\n",
       " 'CCccccccccccccccccccccccccccccc1',\n",
       " 'CCccccccccccccccccccccccccccccccccc1',\n",
       " 'CCcccccccccccccccccccccccccccccc',\n",
       " 'CCcccccccccccccccccccccccc1',\n",
       " 'CCccccccccccccccccccccccc1',\n",
       " 'CCccccccccccccccccccccccc1',\n",
       " 'CCccccccccccccccccccccccc1',\n",
       " 'CCcccccccccccccccccccccc1',\n",
       " 'CCccccccccccccccccccccccc1',\n",
       " 'CCcccccccccccccccccccccccccccccc',\n",
       " 'CCccccccccccccccccccccccccc1',\n",
       " 'CCcccccccccccccccccccccc1',\n",
       " 'CCccccccccccccccccccccccccc1',\n",
       " 'CCcccccccccccccccccccccccccccc1',\n",
       " 'CCccccccccccccccccccccccccccccccc',\n",
       " 'CCcccccccccccccccccccccccccccccc',\n",
       " 'CCcccccccccccccccccccccccc1',\n",
       " 'CCccccccccccccccccccccccccccc1',\n",
       " 'CCccccccccccccccccccccccccc1',\n",
       " 'CCccccccccccccccccccccccccc1',\n",
       " 'CCccccccccccccccccccccccccccccc1',\n",
       " 'CCcccccccccccccccccccccccccc1',\n",
       " 'CCcccccccccccccccccccccccccccccccc1',\n",
       " 'CCccccccccccccccccccccccccccccccc1',\n",
       " 'CCccccccccccccccccccccccccccc1',\n",
       " 'CCcccccccccccccccccccccccccccccc1',\n",
       " 'CCccccccccccccccccccccccccccccc',\n",
       " 'CCcccccccccccccccccccccccccccccc',\n",
       " 'CCccccccccccccccccccccccc1',\n",
       " 'CCccccccccccccccccccccccccccccc',\n",
       " 'CCcccccccccccccccccccccccccccc1',\n",
       " 'CCcccccccccccccccccccccc1',\n",
       " 'CCcccccccccccccccccccccccccc1',\n",
       " 'CCccccccccccccccccccccccccccccccccccc1',\n",
       " 'CCccccccccccccccccccccccccccccccc',\n",
       " 'CCcccccccccccccccccccccccccccccccc']"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b6ec9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f0ddc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86fc378",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0fc0781",
   "metadata": {},
   "outputs": [],
   "source": [
    "#criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = args.lr)\n",
    "if DYN_LR:\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', \n",
    "                                factor = 0.8, \n",
    "                                patience = 3,\n",
    "                                min_lr = 0.0001)\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(X_train, \n",
    "                                        batch_size=args.batch_size,\n",
    "                                        shuffle=True, \n",
    "                                        num_workers=6,\n",
    "                                        drop_last = True)\n",
    "\n",
    "val_dataloader = torch.utils.data.DataLoader(X_test, \n",
    "                                        batch_size=args.batch_size,\n",
    "                                        shuffle=True, \n",
    "                                        num_workers=6,\n",
    "                                        drop_last = True)\n",
    "\n",
    "best_epoch_loss_val = 100000\n",
    "x_train_data_per_epoch = X_train.shape[0] - X_train.shape[0]%args.batch_size\n",
    "x_val_data_per_epoch = X_test.shape[0] - X_test.shape[0]%args.batch_size\n",
    "print(\"Div Quantities\",x_train_data_per_epoch,x_val_data_per_epoch)\n",
    "print()\n",
    "print(\"###########################################################################\")\n",
    "for epoch in range(args.epochs):\n",
    "    epoch_loss = 0\n",
    "    print(\"Epoch -- {}\".format(epoch))\n",
    "\n",
    "    for i, data in enumerate(dataloader):\n",
    "\n",
    "        inputs = data.float().to(device)\n",
    "        #inputs = inputs.reshape(batch_size, -1).float()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        input_recon = model(inputs)\n",
    "        latent_loss_val = latent_loss(model.z_mean, model.z_sigma)\n",
    "        loss = F.binary_cross_entropy(input_recon, inputs, size_average=False) + latent_loss_val\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "\n",
    "    print(\"Train Loss -- {:.3f}\".format(epoch_loss/x_train_data_per_epoch))\n",
    "    ###Add 1 Image per Epoch for Visualisation\n",
    "    data_point_sampled = random.randint(0,args.batch_size-1)\n",
    "\n",
    "    print(\"INPUT\",inputs[data_point_sampled])\n",
    "    print(\"OUTPUT\",input_recon[data_point_sampled].reshape(1, 120, len(vocab)))\n",
    "\n",
    "    print(\"Input -- \",onehot_to_smiles(inputs[data_point_sampled].reshape(1, 120, len(vocab)).cpu().detach(), inv_dict))\n",
    "    print(\"Output -- \",onehot_to_smiles(input_recon[data_point_sampled].reshape(1, 120, len(vocab)).cpu().detach(), inv_dict))\n",
    "\n",
    "    #####################Validation Phase\n",
    "    epoch_loss_val = 0\n",
    "    for i, data in enumerate(val_dataloader):\n",
    "\n",
    "        inputs = data.to(device).float()\n",
    "        #inputs = inputs.reshape(batch_size, -1).float()\n",
    "        input_recon = model(inputs)\n",
    "        latent_loss_val = latent_loss(model.z_mean, model.z_sigma)\n",
    "        loss = F.binary_cross_entropy(input_recon, inputs, size_average=False) + latent_loss_val\n",
    "        epoch_loss_val += loss.item()\n",
    "    print(\"Validation Loss -- {:.3f}\".format(epoch_loss_val/x_val_data_per_epoch))\n",
    "    print()\n",
    "    scheduler.step(epoch_loss_val)\n",
    "\n",
    "    ###Add 1 Image per Epoch for Visualisation\n",
    "    #data_point_sampled = random.randint(0,args.batch_size)\n",
    "    #add_img(inputs[data_point_sampled], inv_dict, \"Original_\"+str(epoch))\n",
    "    #add_img(model(inputs[data_point_sampled:data_point_sampled+1]), inv_dict, \"Recon_\"+str(epoch))\n",
    "\n",
    "    checkpoint = {'model': model.state_dict(),\n",
    "                'dict':vocab,\n",
    "                'inv_dict':inv_dict,\n",
    "                }\n",
    "\n",
    "    #Saves when loss is lower than best validation loss till now and all models after 100 epochs\n",
    "    if epoch_loss_recon_val < best_epoch_loss_val or epoch > 100:\n",
    "        torch.save(checkpoint, args.save_loc+'/'+str(epoch)+'checkpoint.pth')\n",
    "    #update best epoch loss\n",
    "    best_epoch_loss_val = min(epoch_loss_val, best_epoch_loss_val)\n",
    "#evaluate(model, X_train, vocab, inv_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec0b281",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a719bdb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1707e19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3efe4baa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02905d3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccea6a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(layer):\n",
    "    '''\n",
    "    Initialize weights based on layer type\n",
    "    \n",
    "    Args:\n",
    "        layer (torch.nn): neural network whose weights to initialize\n",
    "    '''\n",
    "    if type(layer) == nn.Conv1d:\n",
    "        init.normal_(m.weight.data)\n",
    "        m.bias.data.fill_(0.01)\n",
    "    if type(layer) == nn.Linear:\n",
    "        n = m.in_features\n",
    "        y = 1.0/np.sqrt(n)\n",
    "        m.weight.data.uniform_(-y, y)\n",
    "        m.bias.data.fill_(0)\n",
    "    if type(layer) == nn.GRU:\n",
    "        for param in m.parameters():\n",
    "            if len(param.shape) >= 2:\n",
    "                init.orthogonal_(param.data)\n",
    "            else:\n",
    "                init.normal_(param.data)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17810222",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09606af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c87ec2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, d_input, d_hidden_1, d_hidden_2):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.linear1 = nn.Linear(d_input, d_hidden_1)\n",
    "        self.linear2 = nn.Linear(d_hidden_1, d_hidden_2)\n",
    "        self.d_hidden_1 = d_hidden_1\n",
    "        self.d_hidden_2 = d_hidden_2\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = F.relu(self.linear1(x))\n",
    "        z = F.relu(self.linear2(h))\n",
    "        return z\n",
    "\n",
    "class Decoder(torch.nn.Module):\n",
    "    def __init__(self, d_input, d_hidden_1, d_hidden_2):\n",
    "        super(Decoder,self).__init__()\n",
    "        self.linear1 = nn.Linear(d_hidden_2, d_hidden_1)\n",
    "        self.linear2 = nn.Linear(d_hidden_1, d_input)\n",
    "\n",
    "    def forward(self, z):\n",
    "        h = F.relu(self.linear1(z))\n",
    "        x = F.relu(self.linear2(h))\n",
    "        return x\n",
    "\n",
    "class VAE(nn.Module):\n",
    "\n",
    "    def __init__(self, encoder, decoder, d_latent):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.d_latent = d_latent\n",
    "\n",
    "        self.d_hidden_1 = self.encoder.d_hidden_1\n",
    "        self.d_hidden_2 = self.encoder.d_hidden_2\n",
    "\n",
    "        self._enc_mu = torch.nn.Linear(self.d_hidden_2, self.d_latent)\n",
    "        self._enc_log_var = torch.nn.Linear(self.d_hidden_2, self.d_latent)\n",
    "\n",
    "    def _sample_latent(self, h_enc):\n",
    "        '''\n",
    "        Sample from latent space, z ~ N(μ, σ**2)\n",
    "        '''\n",
    "        mu = self._enc_mu(h_enc)\n",
    "        log_var = self._enc_log_var(h_enc)\n",
    "        sigma = torch.exp(log_var / 2)\n",
    "\n",
    "        #epsilon = torch.randn(sigma.size()).float()\n",
    "        epsilon = torch.randn_like(sigma).float()\n",
    "\n",
    "        self.z_mean = mu\n",
    "        self.z_sigma = sigma\n",
    "\n",
    "        # use the reparameterization trick\n",
    "        return mu + sigma * epsilon\n",
    "\n",
    "    def forward(self, state):\n",
    "        h_enc = self.encoder(state)\n",
    "        z = self._sample_latent(h_enc)\n",
    "        x_recon = self.decoder(z)\n",
    "        return x_recon\n",
    "\n",
    "    def get_num_params(self):\n",
    "        print(\"Encoder--\",sum(p.numel() for p in self.encoder.parameters() if p.requires_grad))\n",
    "        print(\"Decoder--\",sum(p.numel() for p in self.decoder.parameters() if p.requires_grad))\n",
    "        print(\"Total--\",sum(p.numel() for p in self.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7df934",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41122c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = train_oh.shape[2]\n",
    "input_dim = train_oh.shape[1] * vocab_size\n",
    "d_hidden_1 = 200\n",
    "d_hidden_2 = 120\n",
    "d_latent = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef43b37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "5c7733bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_bidir = True\n",
    "q_d_h = 256\n",
    "q_n_layers = 1\n",
    "q_dropout = 0.5\n",
    "d_n_layers = 3\n",
    "d_dropout = 0\n",
    "d_z = 128\n",
    "d_d_h = 512\n",
    "# from data import *\n",
    "\n",
    "class MolVAE(nn.Module):\n",
    "  def __init__(self, char, vector):\n",
    "    super().__init__()\n",
    "    self.vocabulary = vocab\n",
    "    self.vector = vector\n",
    "    \n",
    "    n_vocab, d_emb = len(vocab), vector.size(1)\n",
    "    self.x_emb = nn.Embedding(n_vocab, d_emb, char2idx['<pad>'])\n",
    "    self.x_emb.weight.data.copy_(vector)\n",
    "  \n",
    "    # encoder\n",
    "    self.encoder_rnn = nn.GRU(d_emb,q_d_h,num_layers=q_n_layers,batch_first=True,\n",
    "                              dropout=q_dropout if q_n_layers > 1 else 0,bidirectional=q_bidir)\n",
    "    q_d_last = q_d_h * (2 if q_bidir else 1)\n",
    "    self.q_mu = nn.Linear(q_d_last, d_z)\n",
    "    self.q_logvar = nn.Linear(q_d_last, d_z)\n",
    "  \n",
    "    # decoder\n",
    "    self.decoder_rnn = nn.GRU(d_emb + d_z,d_d_h,num_layers=d_n_layers,batch_first=True,\n",
    "                              dropout=d_dropout if d_n_layers > 1 else 0)\n",
    "    self.decoder_latent = nn.Linear(d_z, d_d_h)\n",
    "    self.decoder_fullyc = nn.Linear(d_d_h, n_vocab)\n",
    "  \n",
    "    # save model parameters as nn.ModuleList\n",
    "    self.encoder = nn.ModuleList([self.encoder_rnn,self.q_mu,self.q_logvar])\n",
    "    self.decoder = nn.ModuleList([self.decoder_rnn,self.decoder_latent,self.decoder_fullyc])\n",
    "    self.vae = nn.ModuleList([self.x_emb,self.encoder,self.decoder])\n",
    "    \n",
    "  @property\n",
    "  def device(self):\n",
    "    return next(self.parameters()).device\n",
    "\n",
    "  def string2tensor(self, string, device='model'):\n",
    "    ids = convert_str2num(string, add_bos=True, add_eos=True)\n",
    "    tensor = torch.tensor(ids, dtype=torch.long,device=self.device if device == 'model' else device)\n",
    "    return tensor\n",
    "\n",
    "  def tensor2string(self, tensor):\n",
    "    ids = tensor.tolist()\n",
    "    string = convert_num2str(ids, rem_bos=True, rem_eos=True)\n",
    "    return string\n",
    "  \n",
    "  def forward(self,x):\n",
    "    z, kl_loss = self.forward_encoder(x)\n",
    "    recon_loss = self.forward_decoder(x, z)\n",
    "    print(\"forward\")\n",
    "    return kl_loss, recon_loss\n",
    "  \n",
    "  def forward_encoder(self,x):\n",
    "    x = [self.x_emb(i_x) for i_x in x]\n",
    "    x = nn.utils.rnn.pack_sequence(x)\n",
    "    _, h = self.encoder_rnn(x, None)\n",
    "    h = h[-(1 + int(self.encoder_rnn.bidirectional)):]\n",
    "    h = torch.cat(h.split(1), dim=-1).squeeze(0)\n",
    "    mu, logvar = self.q_mu(h), self.q_logvar(h)\n",
    "    eps = torch.randn_like(mu)\n",
    "    z = mu + (logvar / 2).exp() * eps\n",
    "    kl_loss = 0.5 * (logvar.exp() + mu ** 2 - 1 - logvar).sum(1).mean()\n",
    "    return z, kl_loss\n",
    "  \n",
    "  def forward_decoder(self,x, z):\n",
    "    lengths = [len(i_x) for i_x in x]\n",
    "    x = nn.utils.rnn.pad_sequence(x, batch_first=True, padding_value= char2idx['<pad>'])\n",
    "    x_emb = self.x_emb(x)\n",
    "    z_0 = z.unsqueeze(1).repeat(1, x_emb.size(1), 1)\n",
    "    x_input = torch.cat([x_emb, z_0], dim=-1)\n",
    "    x_input = nn.utils.rnn.pack_padded_sequence(x_input, lengths, batch_first=True)\n",
    "    h_0 = self.decoder_latent(z)\n",
    "    h_0 = h_0.unsqueeze(0).repeat(self.decoder_rnn.num_layers, 1, 1)\n",
    "    output, _ = self.decoder_rnn(x_input, h_0)\n",
    "    output, _ = nn.utils.rnn.pad_packed_sequence(output, batch_first=True)\n",
    "    y = self.decoder_fullyc(output)\n",
    "    \n",
    "    recon_loss = F.cross_entropy(y[:, :-1].contiguous().view(-1, y.size(-1)),\n",
    "                                 x[:, 1:].contiguous().view(-1),ignore_index= char2idx['<pad>'])\n",
    "    return recon_loss\n",
    "  \n",
    "  def sample_z_prior(self,n_batch):\n",
    "    return torch.randn(n_batch,self.q_mu.out_features,device= self.x_emb.weight.device)\n",
    "\n",
    "  def sample(self,n_batch, max_len=100, z=None, temp=1.0):\n",
    "    with torch.no_grad():\n",
    "      if z is None:\n",
    "        z = self.sample_z_prior(n_batch)\n",
    "        z = z.to(self.device)\n",
    "        z_0 = z.unsqueeze(1)\n",
    "        h = self.decoder_latent(z)\n",
    "        h = h.unsqueeze(0).repeat(self.decoder_rnn.num_layers, 1, 1)\n",
    "        w = torch.tensor(char2idx['<bos>'], device=self.device).repeat(n_batch)\n",
    "        x = torch.tensor([char2idx['<pad>']], device=device).repeat(n_batch, max_len)\n",
    "        x[:, 0] = char2idx['<bos>']\n",
    "        end_pads = torch.tensor([max_len], device=self.device).repeat(n_batch)\n",
    "        eos_mask = torch.zeros(n_batch, dtype=torch.uint8, device=self.device)\n",
    "\n",
    "        for i in range(1, max_len):\n",
    "          x_emb = self.x_emb(w).unsqueeze(1)\n",
    "          x_input = torch.cat([x_emb, z_0], dim=-1)\n",
    "\n",
    "          o, h = self.decoder_rnn(x_input, h)\n",
    "          y = self.decoder_fullyc(o.squeeze(1))\n",
    "          y = F.softmax(y / temp, dim=-1)\n",
    "\n",
    "          w = torch.multinomial(y, 1)[:, 0]\n",
    "          x[~eos_mask, i] = w[~eos_mask]\n",
    "          i_eos_mask = ~eos_mask & (w == char2idx['<eos>'])\n",
    "          end_pads[i_eos_mask] = i + 1\n",
    "          eos_mask = eos_mask | i_eos_mask\n",
    "          \n",
    "          new_x = []\n",
    "          for i in range(x.size(0)):\n",
    "            new_x.append(x[i, :end_pads[i]])\n",
    "\n",
    "    return [self.tensor2string(i_x) for i_x in new_x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "4e563073",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = torch.tensor(train_idx, dtype=torch.long, device=self.device if device == 'model' else device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "44a81def",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 50])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "40e5ea85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "CHARSET = [' ', '#', '(', ')', '+', '-', '/', '1', '2', '3', '4', '5', '6', '7',\n",
    "           '8', '=', '@', 'B', 'C', 'F', 'H', 'I', 'N', 'O', 'P', 'S', '[', '\\\\', ']',\n",
    "           'c', 'l', 'n', 'o', 'r', 's']\n",
    "\n",
    "class OneHotFeaturizer(object):\n",
    "    def __init__(self, charset=CHARSET, padlength=120):\n",
    "        self.charset = CHARSET\n",
    "        self.pad_length = padlength\n",
    "\n",
    "    def featurize(self, smiles):\n",
    "        return np.array([self.one_hot_encode(smi) for smi in smiles])\n",
    "\n",
    "    def one_hot_array(self, i):\n",
    "        return [int(x) for x in [ix == i for ix in range(len(self.charset))]]\n",
    "\n",
    "    def one_hot_index(self, c):\n",
    "        return self.charset.index(c)\n",
    "\n",
    "    def pad_smi(self, smi):\n",
    "        return smi.ljust(self.pad_length)\n",
    "\n",
    "    def one_hot_encode(self, smi):\n",
    "        return np.array([\n",
    "            self.one_hot_array(self.one_hot_index(x)) for x in self.pad_smi(smi)\n",
    "            ])\n",
    "\n",
    "    def one_hot_decode(self, z):\n",
    "        z1 = []\n",
    "        for i in range(len(z)):\n",
    "            s = ''\n",
    "            for j in range(len(z[i])):\n",
    "                oh = np.argmax(z[i][j])\n",
    "                s += self.charset[oh]\n",
    "            z1.append([s.strip()])\n",
    "        return z1\n",
    "\n",
    "    def decode_smiles_from_index(self, vec):\n",
    "        return ''.join(map(lambda x: CHARSET[x], vec)).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "a6f6e921",
   "metadata": {},
   "outputs": [],
   "source": [
    "oh_smiles = ohf.featurize(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "583773aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 120, 37)\n"
     ]
    }
   ],
   "source": [
    "ohf = OneHotFeaturizer()\n",
    "oh_smiles = ohf.featurize(X_train)\n",
    "print(oh_smiles.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "6e8613ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class MolecularVAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MolecularVAE, self).__init__()\n",
    "\n",
    "        # The input filter dim should be 35\n",
    "        #  corresponds to the size of CHARSET\n",
    "        self.conv1d1 = nn.Conv1d(37, 9, kernel_size=9)  \n",
    "        self.conv1d2 = nn.Conv1d(9, 9, kernel_size=9)\n",
    "        self.conv1d3 = nn.Conv1d(9, 10, kernel_size=11)\n",
    "        self.fc0 = nn.Linear(940, 435)\n",
    "        self.fc11 = nn.Linear(435, 292)\n",
    "        self.fc12 = nn.Linear(435, 292)\n",
    "\n",
    "        self.fc2 = nn.Linear(292, 292)\n",
    "        self.gru = nn.GRU(292, 501, 3, batch_first=True)\n",
    "        self.fc3 = nn.Linear(501, 37)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = F.relu(self.conv1d1(x))\n",
    "        h = F.relu(self.conv1d2(h))\n",
    "        h = F.relu(self.conv1d3(h))\n",
    "        h = h.view(h.size(0), -1)\n",
    "        h = F.selu(self.fc0(h))\n",
    "        return self.fc11(h), self.fc12(h)\n",
    "\n",
    "    def reparametrize(self, mu, logvar):\n",
    "        if self.training:\n",
    "            std = torch.exp(0.5 * logvar)\n",
    "            eps = 1e-2 * torch.randn_like(std)\n",
    "            w = eps.mul(std).add_(mu)\n",
    "            return w\n",
    "        else:\n",
    "            return mu\n",
    "\n",
    "    def decode(self, z):\n",
    "        z = F.selu(self.fc2(z))\n",
    "        z = z.view(z.size(0), 1, z.size(-1)).repeat(1, 60, 1)\n",
    "        out, h = self.gru(z)\n",
    "        out_reshape = out.contiguous().view(-1, out.size(-1))\n",
    "        y0 = F.softmax(self.fc3(out_reshape), dim=1)\n",
    "        y = y0.contiguous().view(out.size(0), -1, y0.size(-1))\n",
    "        return y\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparametrize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "3161b7ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jessb\\anaconda3\\envs\\dl_project\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 / 0\t138462.9219\n",
      "train tensor(436.6977, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "2 / 0\t84523.0391\n",
      "train tensor(268.6349, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "3 / 0\t56684.4297\n",
      "train tensor(200.5436, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "4 / 0\t45009.7422\n",
      "train tensor(178.0354, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "5 / 0\t43012.2305\n",
      "train tensor(170.2285, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "6 / 0\t41145.2773\n",
      "train tensor(162.1197, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "7 / 0\t38937.4180\n",
      "train tensor(151.7357, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "8 / 0\t35954.7070\n",
      "train tensor(140.3110, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "9 / 0\t33518.8320\n",
      "train tensor(129.7825, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "10 / 0\t33578.4609\n",
      "train tensor(133.5314, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "11 / 0\t32651.2031\n",
      "train tensor(126.3311, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "12 / 0\t31844.3652\n",
      "train tensor(124.3849, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "13 / 0\t30241.6758\n",
      "train tensor(121.1066, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "14 / 0\t29662.7051\n",
      "train tensor(119.0280, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "15 / 0\t30122.7754\n",
      "train tensor(118.2796, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "16 / 0\t29718.8125\n",
      "train tensor(116.8564, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "17 / 0\t28470.0762\n",
      "train tensor(115.4255, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "18 / 0\t29148.4805\n",
      "train tensor(113.0484, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "19 / 0\t27743.9805\n",
      "train tensor(110.1438, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "20 / 0\t27312.7656\n",
      "train tensor(114.3387, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "21 / 0\t29847.3613\n",
      "train tensor(115.4206, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "22 / 0\t28924.2402\n",
      "train tensor(113.1097, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "23 / 0\t28962.1035\n",
      "train tensor(111.7132, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "24 / 0\t28239.9434\n",
      "train tensor(110.1986, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "25 / 0\t27827.3711\n",
      "train tensor(108.8534, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "26 / 0\t27141.6289\n",
      "train tensor(108.0727, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "27 / 0\t26726.8926\n",
      "train tensor(106.8143, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "28 / 0\t26778.6172\n",
      "train tensor(106.4525, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "29 / 0\t26397.5059\n",
      "train tensor(106.1613, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "30 / 0\t26770.2793\n",
      "train tensor(105.6201, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "31 / 0\t26588.6113\n",
      "train tensor(108.4048, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "32 / 0\t27408.7461\n",
      "train tensor(107.3622, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "33 / 0\t27417.9297\n",
      "train tensor(106.2697, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "34 / 0\t26498.5391\n",
      "train tensor(107.0458, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "35 / 0\t28218.1348\n",
      "train tensor(113.7868, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "36 / 0\t27570.4062\n",
      "train tensor(114.6622, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "37 / 0\t26696.5645\n",
      "train tensor(110.0340, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "38 / 0\t27283.4551\n",
      "train tensor(108.2186, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "39 / 0\t27350.2344\n",
      "train tensor(107.6000, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "40 / 0\t26674.5312\n",
      "train tensor(106.5622, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "41 / 0\t25981.2070\n",
      "train tensor(105.5949, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "42 / 0\t26050.6934\n",
      "train tensor(104.9585, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "43 / 0\t25906.8164\n",
      "train tensor(104.3213, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "44 / 0\t25838.4180\n",
      "train tensor(103.9712, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "45 / 0\t25669.0488\n",
      "train tensor(103.6835, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "46 / 0\t25453.0898\n",
      "train tensor(103.4771, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "47 / 0\t25380.2031\n",
      "train tensor(103.5745, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "48 / 0\t25971.8906\n",
      "train tensor(103.9935, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "49 / 0\t26317.2930\n",
      "train tensor(104.3745, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "50 / 0\t26636.6934\n",
      "train tensor(103.8100, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "51 / 0\t25239.1992\n",
      "train tensor(102.3556, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "52 / 0\t25606.2617\n",
      "train tensor(103.9035, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "53 / 0\t26428.6211\n",
      "train tensor(104.6377, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "54 / 0\t25909.3340\n",
      "train tensor(106.1162, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "55 / 0\t28015.2461\n",
      "train tensor(121.8054, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "56 / 0\t26440.7051\n",
      "train tensor(112.8924, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "57 / 0\t27592.1465\n",
      "train tensor(109.3635, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "58 / 0\t28360.6719\n",
      "train tensor(109.3183, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "59 / 0\t26159.1055\n",
      "train tensor(106.4948, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "60 / 0\t26800.4844\n",
      "train tensor(106.2753, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "61 / 0\t26127.8926\n",
      "train tensor(105.4089, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "62 / 0\t25804.2051\n",
      "train tensor(104.4625, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "63 / 0\t26285.9551\n",
      "train tensor(104.1185, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "64 / 0\t25741.0938\n",
      "train tensor(103.5962, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "65 / 0\t25995.0059\n",
      "train tensor(103.1198, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "66 / 0\t25860.5547\n",
      "train tensor(102.8682, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "67 / 0\t25517.5508\n",
      "train tensor(102.5859, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "68 / 0\t25762.6445\n",
      "train tensor(102.2806, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "69 / 0\t25686.3750\n",
      "train tensor(102.0770, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "70 / 0\t25780.0312\n",
      "train tensor(101.9794, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "71 / 0\t25234.7559\n",
      "train tensor(101.7598, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "72 / 0\t25305.7031\n",
      "train tensor(101.4969, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "73 / 0\t25466.2734\n",
      "train tensor(102.7323, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "74 / 0\t25499.7188\n",
      "train tensor(102.5852, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "75 / 0\t25404.6211\n",
      "train tensor(102.5027, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "76 / 0\t25678.8184\n",
      "train tensor(102.1219, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "77 / 0\t25311.7930\n",
      "train tensor(102.0050, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "78 / 0\t25237.5547\n",
      "train tensor(102.4633, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "79 / 0\t25392.1348\n",
      "train tensor(101.5322, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "80 / 0\t25729.3945\n",
      "train tensor(102.6464, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "81 / 0\t25365.0801\n",
      "train tensor(102.6999, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "82 / 0\t25744.7656\n",
      "train tensor(101.2511, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "83 / 0\t26272.5820\n",
      "train tensor(103.3312, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "84 / 0\t26562.8105\n",
      "train tensor(116.6670, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "85 / 0\t26887.2461\n",
      "train tensor(108.9447, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "86 / 0\t26799.2812\n",
      "train tensor(106.6720, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "87 / 0\t26038.9004\n",
      "train tensor(104.7838, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "88 / 0\t25983.2500\n",
      "train tensor(103.6228, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "89 / 0\t25697.1328\n",
      "train tensor(102.7792, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "90 / 0\t25674.5879\n",
      "train tensor(102.3803, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "91 / 0\t25213.1074\n",
      "train tensor(102.0337, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "92 / 0\t25441.6426\n",
      "train tensor(101.7463, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "93 / 0\t25267.3652\n",
      "train tensor(101.5748, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "94 / 0\t25567.7012\n",
      "train tensor(101.3569, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "95 / 0\t25812.9824\n",
      "train tensor(101.4334, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "96 / 0\t24821.3594\n",
      "train tensor(101.5616, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "97 / 0\t25153.0957\n",
      "train tensor(101.5716, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "98 / 0\t25867.0059\n",
      "train tensor(104.7542, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "99 / 0\t26664.0195\n",
      "train tensor(106.5081, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "100 / 0\t25987.8789\n",
      "train tensor(102.8024, device='cuda:0', grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import model_selection\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "# import h5py\n",
    "\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    BCE = F.binary_cross_entropy(recon_x, x, size_average=False)\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return BCE + KLD\n",
    "\n",
    "X = oh_smiles.astype(np.float32)\n",
    "\n",
    "train = torch.utils.data.TensorDataset(torch.from_numpy(X))\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size=250, shuffle=True)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "epochs = 100\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model = MolecularVAE().to(device)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, data in enumerate(train_loader):\n",
    "        data = data[0].transpose(1,2).to(device)\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        loss = loss_function(recon_batch, data.transpose(1,2), mu, logvar)\n",
    "        loss.backward()\n",
    "        train_loss += loss\n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f'{epoch} / {batch_idx}\\t{loss:.4f}')\n",
    "    print('train', train_loss / len(train_loader.dataset))\n",
    "    return train_loss / len(train_loader.dataset)\n",
    "\n",
    "def test(epoch):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    for batch_idx, data in enumerate(test_loader):\n",
    "        data = data[0].transpose(1,2).to(device)\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        test_loss += loss_function(recon_batch, data.transpose(1,2), mu, logvar).item()\n",
    "    print('test', test_loss / len(test_loader))\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train_loss = train(epoch)\n",
    "    if epoch % 1 == 0:\n",
    "        torch.save(model.state_dict(),\n",
    "                './weights/vae-{:03d}-{}.pth'.format(epoch, train_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "fb285bdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 120, 37])"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "f5f078d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_vec = torch.from_numpy(oh.featurize([start]).astype(np.float32)).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "decbcfd8",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [9, 37, 9], expected input[1, 120, 37] to have 37 channels, but got 120 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-152-594568d67447>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mstart_vec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moh\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeaturize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cuda'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mrecon_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart_vec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecon_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\dl_project\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-138-ebcd2859d4d0>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m         \u001b[0mmu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogvar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m         \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreparametrize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogvar\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogvar\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-138-ebcd2859d4d0>\u001b[0m in \u001b[0;36mencode\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m         \u001b[0mh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv1d1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m         \u001b[0mh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv1d2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[0mh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv1d3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\dl_project\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\dl_project\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    299\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    300\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 301\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\dl_project\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    296\u001b[0m                             _single(0), self.dilation, self.groups)\n\u001b[0;32m    297\u001b[0m         return F.conv1d(input, weight, bias, self.stride,\n\u001b[1;32m--> 298\u001b[1;33m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[0;32m    299\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    300\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Given groups=1, weight of size [9, 37, 9], expected input[1, 120, 37] to have 37 channels, but got 120 channels instead"
     ]
    }
   ],
   "source": [
    "model = MolecularVAE()\n",
    "model.load_state_dict(torch.load('./weights/vae-100-102.80235290527344.pth'))\n",
    "model.to('cuda')\n",
    "model.eval()\n",
    "\n",
    "start = 'C[C@@H]1CN(C(=O)c2cc(Br)cn2C)CC[C@H]1[NH3+]'\n",
    "start = start.ljust(120)\n",
    "oh = OneHotFeaturizer()\n",
    "start_vec = torch.from_numpy(oh.featurize([start]).astype(np.float32)).to('cuda')\n",
    "\n",
    "recon_x = model(start_vec)[0].cpu().detach().numpy()\n",
    "y = np.argmax(recon_x, axis=2)\n",
    "print(start)\n",
    "print(oh.decode_smiles_from_index(y[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e160a37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275dea92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c94b7f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b88f2f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf093ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349bca27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef267d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, hidden_size, embedding_size, embedding_dim, rnn, num_layers, bidirectional, device): \n",
    "        super(Encoder, self).__init__()        \n",
    "        self.latent_dim = 1024\n",
    "\n",
    "        # Encoder Setup\n",
    "\n",
    "    def forward(self,input_seq):\n",
    "\n",
    "        return mean, logv\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self): \n",
    "        super(Decoder, self).__init__()        \n",
    "        self.latent_dim = 1024\n",
    "\n",
    "        # Decoder Setup\n",
    "\n",
    "    def forward(self, z, actual_input=None):\n",
    "\n",
    "        #generated_sequence must be BATCH x SEQLENGTH, have type long, and contain\n",
    "        #the index form of the generated sequences (smiles strings can be generated\n",
    "        #by passing rows to Lang.indexToSmiles)\n",
    "        return decoder_output, generated_sequence \n",
    "\n",
    "\n",
    "class VAE(nn.Module):      \n",
    "    def __init__(self): #all sorts of hyper parameters should be passed here            \n",
    "        super(VAE, self).__init__()        \n",
    "          \n",
    "        self.latent_dim = 1024\n",
    "       \n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder()\n",
    "                                                                                          \n",
    "    def forward(self, input_seq):\n",
    "\n",
    "        mean, logv = self.encoder(input_seq)\n",
    "        \n",
    "        # calculate z\n",
    "\n",
    "        decoder_output, generated_sequence = self.decoder(z,actual_input=input_seq)\n",
    "                \n",
    "        return decoder_output, generated_sequence, (mean, logv, z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d68e946",
   "metadata": {},
   "source": [
    "# Snippets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d2f161c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "100%|████████████████████████████████████| 75.3M/75.3M [00:08<00:00, 8.63MiB/s]\n",
      "Loading...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from tdc.generation import MolGen\n",
    "data = MolGen(name = 'MOSES')\n",
    "split = data.get_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3dcaa862",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tdc.chem_utils import MolConvert\n",
    "converter = MolConvert(src = 'SMILES', dst = 'SELFIES')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dcc0c8b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tdc.chem_utils.featurize.molconvert.MolConvert at 0x2a4c5a8c9e8>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "converter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0ee417",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e81aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "## http://bits.csb.pitt.edu/mscbio2066/assign7/\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "import gzip\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.nn import init\n",
    "import argparse\n",
    "\n",
    "class Lang:\n",
    "    '''Predefined mapping from characters to indices for our\n",
    "    reduced alphabet of SMILES with methods for converting.\n",
    "    You must use this mapping.'''\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.chartoindex = {'EOS': 0,'SOS': 1, 'C': 2, '(': 3,\n",
    "                '=': 4, 'O': 5, ')': 6, '[': 7, '-': 8, ']': 9,\n",
    "                'N': 10, '+': 11, '1': 12, 'P': 13, '2': 14,'3': 15,\n",
    "                '4': 16, 'S': 17, '#': 18, '5': 19,'6': 20, '7': 21,\n",
    "                'H': 22, 'I': 23, 'B': 24, 'F': 25, '8': 26, '9': 27\n",
    "                } \n",
    "        self.indextochar = {0: 'EOS', 1: 'SOS', 2: 'C', 3: '(',\n",
    "                4: '=', 5: 'O', 6: ')', 7: '[', 8: '-', 9: ']',\n",
    "                10: 'N', 11: '+', 12: '1', 13: 'P', 14: '2', 15: '3',\n",
    "                16: '4', 17: 'S', 18: '#', 19: '5', 20: '6', 21: '7',\n",
    "                22: 'H', 23: 'I', 24: 'B', 25: 'F', 26: '8', 27: '9'\n",
    "                }\n",
    "        self.nchars = 28\n",
    "        \n",
    "    def indexesFromSMILES(self, smiles_str):\n",
    "        index_list = [self.chartoindex[char] for char in smiles_str]\n",
    "        index_list.append(self.chartoindex[\"EOS\"])\n",
    "        return np.array(index_list, dtype=np.uint8)\n",
    "        \n",
    "    def indexToSmiles(self,indices):\n",
    "        '''convert list of indices into a smiles string'''\n",
    "        smiles_str = ''.join(list(map(lambda x: self.indextochar[int(x)] if x != 0.0 else 'E',indices)))\n",
    "        return smiles_str.split('E')[0] #Only want values before output 'EOS' token\n",
    "\n",
    "\n",
    "class SmilesDataset(torch.utils.data.Dataset):\n",
    "    '''Dataset that reads in a gzipped smiles file and converts to a\n",
    "    numpy array representation.  Note we encountered memory usage issues\n",
    "    when using variable sequence length batches and so use a fixed size.\n",
    "    There are likely more memory efficient ways to store this data.'''\n",
    "    def __init__(self,data_path,max_length=150):\n",
    "        self.max_length = max_length\n",
    "        self.language = Lang()\n",
    "        #TODO - for faster training you will want to preprocess\n",
    "        #the training set and read in this processed file instead\n",
    "        #for faster initialization\n",
    "        with gzip.open(data_path,'rt') as f:\n",
    "            N = sum(1 for line in f)\n",
    "        self.examples = np.zeros((N,max_length),dtype=np.uint8)\n",
    "        with gzip.open(data_path,'rt') as f:\n",
    "            for i,line in enumerate(f):\n",
    "                example = line.rstrip()\n",
    "                ex = self.language.indexesFromSMILES(example)\n",
    "                self.examples[i][:len(ex)] = ex\n",
    "                \n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.examples[idx], dtype=torch.long)\n",
    "    \n",
    "    def getIndexToChar(self):\n",
    "        return self.language.indextochar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87306db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, hidden_size, embedding_size, embedding_dim,rnn, num_layers, bidirectional, device): \n",
    "        super(Encoder, self).__init__()        \n",
    "        self.latent_dim = 1024\n",
    "\n",
    "        # Encoder Setup\n",
    "\n",
    "    def forward(self,input_seq):\n",
    "\n",
    "        return mean, logv\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self): \n",
    "        super(Decoder, self).__init__()        \n",
    "        self.latent_dim = 1024\n",
    "\n",
    "        # Decoder Setup\n",
    "\n",
    "    def forward(self, z, actual_input=None):\n",
    "\n",
    "        #generated_sequence must be BATCH x SEQLENGTH, have type long, and contain\n",
    "        #the index form of the generated sequences (smiles strings can be generated\n",
    "        #by passing rows to Lang.indexToSmiles)\n",
    "        return decoder_output, generated_sequence \n",
    "\n",
    "\n",
    "class VAE(nn.Module):      \n",
    "    def __init__(self): #all sorts of hyper parameters should be passed here            \n",
    "        super(VAE, self).__init__()        \n",
    "          \n",
    "        self.latent_dim = 1024\n",
    "       \n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder()\n",
    "                                                                                          \n",
    "    def forward(self, input_seq):\n",
    "\n",
    "        mean, logv = self.encoder(input_seq)\n",
    "        \n",
    "        # calculate z\n",
    "\n",
    "        decoder_output, generated_sequence = self.decoder(z,actual_input=input_seq)\n",
    "                \n",
    "        return decoder_output, generated_sequence, (mean, logv, z)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    LATENT_DIM = 1024\n",
    "\n",
    "    parser = argparse.ArgumentParser('Train a Variational Autoencoder')\n",
    "    parser.add_argument('--train_data','-T',required=True,help='data to train the VAE with')\n",
    "    parser.add_argument('--out',default='vae_generate.pth',help='File to save generate function to')\n",
    "    #more arguments...\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "\n",
    "    dataset = SmilesDataset(args.train_data)\n",
    "\n",
    "    language_mapping = dataset.getIndexToChar()\n",
    "\n",
    "    vae = VAE(args.hidden_size,len(language_mapping),args.embedding_dim,args.rnn_type,args.num_layers,args.bidirectional, device='cuda').to('cuda')\n",
    "\n",
    "    #TRAIN THE MODEL\n",
    "\n",
    "\n",
    "    # This will create the file that you will submit to evaluate SMILES generated from a normal distribution\n",
    "    # If you want to implement your decoding as a distinct method in your vae module,\n",
    "    # you will need to wrap calling it in another module and trace that.\n",
    "    z_1 = torch.normal(0, 1, size=(1, LATENT_DIM),device='cuda')\n",
    "    with torch.no_grad():\n",
    "        vae.decoder.eval()\n",
    "        traced = torch.jit.trace(vae.decoder, z_1.to('cuda'))\n",
    "\n",
    "        torch.jit.save(traced,args.out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
